{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "representative-rabbit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.utils.data as data\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch.utils\n",
    "import torchtext\n",
    "from torchtext.data import Example\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from torch_geometric.utils import dense_to_sparse, to_dense_adj, to_dense_batch\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import networkx as nx\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import h5py\n",
    "from collections import defaultdict\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "spatial-prototype",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = \"/local/home/rchan31/SIGIR/detailed-experiments/data\"\n",
    "\n",
    "IMG_FTR_FOLDER = os.path.join(ROOT_PATH, \"img_ftrs\")\n",
    "OBJ_EMBEDDING_FILE = os.path.join(ROOT_PATH, \"gqa_50_singular_predicates_embeddings_300d\")\n",
    "\n",
    "TRAIN_BBOX_FOLDER = os.path.join(ROOT_PATH, \"bbox_data_gqa_pytorch_normalized2\")\n",
    "TRAIN_SG_FILE = os.path.join(ROOT_PATH, \"2gqa_rel_annotations_train_compatible_for_scene_graph.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "expired-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # toDO:\n",
    "# 1. use unfiltered sg data, box features, pred features\n",
    "# 2. use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "varying-physics",
   "metadata": {},
   "outputs": [],
   "source": [
    "GT_VAL_ROOT = \"/local/home/rchan31/SIGIR/detailed-experiments/data/GT_VAL_DATA/\"\n",
    "\n",
    "VAL_GT_BBOX_FOLDER = os.path.join(GT_VAL_ROOT, \"gt_val_bboxes\")\n",
    "VAL_GT_SG_FILE = os.path.join(GT_VAL_ROOT, \"scene_graph_val_gt.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "funny-private",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR_ROOT = \"/local/home/rchan31/SIGIR/detailed-experiments/SAVED_DIRS/\"\n",
    "\n",
    "SAVE_DIR = os.path.join(SAVE_DIR_ROOT, \"saved-exp01\")\n",
    "writer = SummaryWriter(SAVE_DIR + '/runs_1/')\n",
    "\n",
    "RESET_MODEL = False\n",
    "\n",
    "if RESET_MODEL:\n",
    "    shutil.rmtree(SAVE_DIR)\n",
    "    \n",
    "os.makedirs(SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-identity",
   "metadata": {},
   "source": [
    "# Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "western-terrorist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943000\n"
     ]
    }
   ],
   "source": [
    "TRAIN_QST_FILE = \"/local/home/rchan31/SIGIR/detailed-experiments/data/QUESTIONS/train_balanced_questions.json\"\n",
    "\n",
    "train_qas_data = json.load( open(TRAIN_QST_FILE, 'r') )\n",
    "print(len(train_qas_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "specialized-shade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74942\n"
     ]
    }
   ],
   "source": [
    "TRAIN_SG_DATA = json.load( open(TRAIN_SG_FILE, \"r\") )\n",
    "print(len(TRAIN_SG_DATA) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-cattle",
   "metadata": {},
   "source": [
    "# Load val data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sorted-agriculture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132062\n"
     ]
    }
   ],
   "source": [
    "VAL_QST_FILE = \"/local/home/rchan31/SIGIR/detailed-experiments/data/QUESTIONS/val_balanced_questions.json\"\n",
    "\n",
    "val_qas_data = json.load( open(VAL_QST_FILE, 'r') )\n",
    "print(len(val_qas_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "instructional-neighborhood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10696\n"
     ]
    }
   ],
   "source": [
    "VAL_GT_SG_DATA = json.load( open(VAL_GT_SG_FILE, \"r\") )\n",
    "print(len(VAL_GT_SG_DATA) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-meaning",
   "metadata": {},
   "source": [
    "# Data processing & build vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ordered-gentleman",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(text):\n",
    "    # Replace annoying unicode with a space\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    # The following replacements are suggested in the paper\n",
    "    # BidAF (Seo et al., 2016)\n",
    "    text = text.replace(\"''\", '\" ')\n",
    "    text = text.replace(\"``\", '\" ')\n",
    "\n",
    "    # Space out punctuation\n",
    "    space_list = \"!\\\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\"\n",
    "    space_list = \"!\\\"#$%&()*+,./:;<=>?@[\\\\]^_`{|}~\"\n",
    "    text = text.translate(str.maketrans({key: \" {0} \".format(key) for key in space_list}))\n",
    "\n",
    "    # space out singlequotes a bit better (and like stanford)\n",
    "    text = text.replace(\"'\", \" '\")\n",
    "    \n",
    "    # use any APIs\n",
    "    text = text.replace('\\t', '').replace('\\n', '').lower().strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "finnish-density",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete data (train + val) size :  1075062\n",
      "Processing train vocab\n",
      "Finished  0\n",
      "Finished  100000\n",
      "Finished  200000\n",
      "Finished  300000\n",
      "Finished  400000\n",
      "Finished  500000\n",
      "Finished  600000\n",
      "Finished  700000\n",
      "Finished  800000\n",
      "Finished  900000\n",
      "Processing val vocab\n",
      "Finished  0\n",
      "Finished  100000\n",
      "======= Question ========\n",
      "2874 1676\n",
      "torch.Size([2874, 300]) torch.Size([1676, 300])\n"
     ]
    }
   ],
   "source": [
    "PAD_TOKEN = '<PAD>'\n",
    "SOS_TOKEN = '<SOS>'\n",
    "UNK_TOKEN = '<UNK>'\n",
    "EOS_TOKEN = '<EOS>'\n",
    "\n",
    "print(\"complete data (train + val) size : \", len(train_qas_data) + len(val_qas_data) )\n",
    "\n",
    "MAX_QUESTION_LENGTH = 14\n",
    "MAX_ANSWER_LEN = 1\n",
    "\n",
    "QUE_TEXT = torchtext.data.Field(sequential=True, \n",
    "                                tokenize=lambda x: x.split(),\n",
    "                                # init_token=SOS_WORD,\n",
    "                                eos_token=EOS_TOKEN,\n",
    "                                pad_token=PAD_TOKEN,\n",
    "                                include_lengths=True,\n",
    "                                batch_first=True,\n",
    "                                fix_length=MAX_QUESTION_LENGTH, # + 1\n",
    "                                lower=True)\n",
    "\n",
    "ANS_TEXT = torchtext.data.Field(sequential=True, \n",
    "                                tokenize=lambda x: x.split(),                                \n",
    "                                # eos_token=EOS_WORD,\n",
    "                                init_token=SOS_TOKEN,\n",
    "                                pad_token=PAD_TOKEN,\n",
    "                                batch_first=True,\n",
    "                                fix_length=MAX_ANSWER_LEN + 1, # +1 because of attached SOS token at the beginning\n",
    "                                lower=True)\n",
    "\n",
    "example_texts = []\n",
    "\n",
    "FIELDS = [('ans_text', ANS_TEXT), ('que_text', QUE_TEXT)]\n",
    "\n",
    "print(\"Processing train vocab\")\n",
    "for idx, (qas_id, qas_ins) in enumerate(train_qas_data.items()):\n",
    "    q = qas_ins[\"question\"]\n",
    "    a = qas_ins[\"answer\"]\n",
    "    \n",
    "    q = tokenise(q).replace('?', '')\n",
    "    a = tokenise(a).replace('?', '')\n",
    "    \n",
    "    example_texts.append( Example.fromlist([a, q] , FIELDS ) )\n",
    "    \n",
    "    if idx % 100000 == 0:\n",
    "        print(\"Finished \", idx)\n",
    "        \n",
    "print(\"Processing val vocab\")\n",
    "for idx, (qas_id, qas_ins) in enumerate(val_qas_data.items()):\n",
    "    q = qas_ins[\"question\"]\n",
    "    a = qas_ins[\"answer\"]\n",
    "    \n",
    "    q = tokenise(q).replace('?', '')\n",
    "    a = tokenise(a).replace('?', '')\n",
    "    \n",
    "    example_texts.append( Example.fromlist([a, q] , FIELDS ) )\n",
    "    \n",
    "    if idx % 100000 == 0:\n",
    "        print(\"Finished \", idx)\n",
    "\n",
    "torchtext_dataset = torchtext.data.Dataset(example_texts, fields=FIELDS)\n",
    "\n",
    "QUE_TEXT.build_vocab(torchtext_dataset, vectors='glove.6B.300d', vectors_cache='/local/home/rchan31/SIGIR/detailed-experiments/data/vocab_cache')\n",
    "ANS_TEXT.build_vocab(torchtext_dataset, vectors='glove.6B.300d', vectors_cache='/local/home/rchan31/SIGIR/detailed-experiments/data/vocab_cache')\n",
    "\n",
    "print(\"======= Question ========\")\n",
    "print(len(QUE_TEXT.vocab), len(ANS_TEXT.vocab))\n",
    "print(QUE_TEXT.vocab.vectors.size(), ANS_TEXT.vocab.vectors.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "employed-ratio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading train_structured_qas ...\n",
      "897111\n"
     ]
    }
   ],
   "source": [
    "def train_get_qas_for_imgs_present(img_id):\n",
    "    if os.path.isfile( os.path.join(IMG_FTR_FOLDER, \"{}.json\".format(img_id)) ) and \\\n",
    "        os.path.isfile( os.path.join(TRAIN_BBOX_FOLDER, \"{}.json\".format(img_id)) ):\n",
    "        if (img_id in TRAIN_SG_DATA) and (len(TRAIN_SG_DATA[str(img_id)]) != 0):\n",
    "            return True\n",
    "    else: False\n",
    "\n",
    "def train_decouple_q_and_a(qas_data):    \n",
    "    structured_qas = {\"img_id\": [], \"questions\": [], \"answers\": [], \"question_ids\": []}\n",
    "    for idx, (qas_id, qas_ins) in enumerate(qas_data.items()):\n",
    "        img_id = qas_ins['imageId']        \n",
    "        \n",
    "        if not (train_get_qas_for_imgs_present(img_id)): continue\n",
    "        \n",
    "        q = qas_ins[\"question\"]\n",
    "        a = qas_ins[\"answer\"]\n",
    "        \n",
    "        structured_qas['img_id'].append(img_id)\n",
    "        structured_qas['questions'].append(q)\n",
    "        structured_qas['answers'].append(a)\n",
    "        structured_qas['question_ids'].append(qas_id)\n",
    "    \n",
    "    return structured_qas\n",
    "\n",
    "train_structured_qas = train_decouple_q_and_a(train_qas_data)\n",
    "print(\"Finished loading train_structured_qas ...\")\n",
    "\n",
    "print( len(train_structured_qas['answers']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "balanced-insulation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading valid_structured_qas ...\n",
      "125726\n"
     ]
    }
   ],
   "source": [
    "def val_get_qas_for_imgs_present(img_id):\n",
    "    if os.path.isfile( os.path.join(IMG_FTR_FOLDER, \"{}.json\".format(img_id)) ) and \\\n",
    "        os.path.isfile( os.path.join(VAL_GT_BBOX_FOLDER, \"{}.json\".format(img_id)) ):\n",
    "        if (img_id in VAL_GT_SG_DATA) and (len(VAL_GT_SG_DATA[str(img_id)]) != 0):\n",
    "            return True\n",
    "    else: False\n",
    "\n",
    "def val_decouple_q_and_a(qas_data):    \n",
    "    structured_qas = {\"img_id\": [], \"questions\": [], \"answers\": [], \"question_ids\": []}\n",
    "    for idx, (qas_id, qas_ins) in enumerate(qas_data.items()):\n",
    "        img_id = qas_ins['imageId']        \n",
    "        \n",
    "        if not (val_get_qas_for_imgs_present(img_id)): continue\n",
    "        \n",
    "        q = qas_ins[\"question\"]\n",
    "        a = qas_ins[\"answer\"]\n",
    "        \n",
    "        structured_qas['img_id'].append(img_id)\n",
    "        structured_qas['questions'].append(q)\n",
    "        structured_qas['answers'].append(a)\n",
    "        structured_qas['question_ids'].append(qas_id)\n",
    "    \n",
    "    return structured_qas\n",
    "\n",
    "valid_structured_qas = val_decouple_q_and_a(val_qas_data)\n",
    "print(\"Finished loading valid_structured_qas ...\")\n",
    "\n",
    "print( len(valid_structured_qas['answers']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-teacher",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "conscious-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token_idx = QUE_TEXT.vocab.stoi[PAD_TOKEN]\n",
    "NUM_NODES = 40\n",
    "\n",
    "def load_predicate_features_file():\n",
    "    pred_embeddings = []\n",
    "    with open(OBJ_EMBEDDING_FILE, 'r') as pred_file:\n",
    "        all_pred_embeddings = [line.rstrip('\\n') for line in pred_file]\n",
    "        for embedding in all_pred_embeddings:\n",
    "            pred_embeddings.append([float(i) for i in embedding[2:][:-2].split()])\n",
    "    return np.asarray(pred_embeddings)\n",
    "\n",
    "def load_object_visual_features(imageID, data_mode):\n",
    "    if data_mode == 'train':\n",
    "        with open(os.path.join(TRAIN_BBOX_FOLDER, \"%s.json\" % imageID), 'r') as f:\n",
    "            obj_feat_set = json.loads(f.read())\n",
    "        return obj_feat_set\n",
    "    elif data_mode == 'gt_val':\n",
    "        with open(os.path.join(VAL_GT_BBOX_FOLDER, \"%s.json\" % imageID), 'r') as f:\n",
    "            obj_feat_set = json.loads(f.read())\n",
    "        return obj_feat_set\n",
    "    else:\n",
    "        with open(os.path.join(VAL_INF_BBOX_FOLDER, \"%s.json\" % imageID), 'r') as f:\n",
    "            obj_feat_set = json.loads(f.read())\n",
    "        return obj_feat_set\n",
    "\n",
    "def prepare_questions(questions_list):\n",
    "    for q in questions_list:\n",
    "        q = tokenise(q).replace('?', '')\n",
    "        yield q.split()\n",
    "        \n",
    "def prepare_answers(answers_list):\n",
    "    for a in answers_list:\n",
    "        a = tokenise(a).replace('.', '')\n",
    "        yield a.split()\n",
    "\n",
    "        \n",
    "class VQA(torch.utils.data.Dataset):\n",
    "    \"\"\" VQA dataset, open-ended \"\"\"\n",
    "    def __init__(self,\n",
    "                structured_qas,\n",
    "                question_field,\n",
    "                answer_field,\n",
    "                data_mode=\"train\"):\n",
    "        super(VQA, self).__init__()\n",
    "\n",
    "        print(\"Step1 : Data loading\")\n",
    "        print(\"....... {} mode number of data samples : {}\".format(data_mode, len(structured_qas['img_id'])))\n",
    "        self.question_field = question_field\n",
    "        self.answer_field = answer_field\n",
    "\n",
    "        # ========================= Load Answer Vocab =================\n",
    "        self.answers = list(prepare_answers( structured_qas['answers'] ))\n",
    "        self.answers = answer_field.pad(self.answers)\n",
    "        self.answers = answer_field.numericalize(self.answers)\n",
    "        print(\"....... VG Answer data have been PREPARED & ENCODED ...\")\n",
    "\n",
    "        # ========================= Load Question Vocab =================\n",
    "        self.questions_vocab = question_field.vocab\n",
    "        self.questions = list(prepare_questions( structured_qas['questions'] ))\n",
    "        self.questions = question_field.pad(self.questions)\n",
    "        self.questions = question_field.numericalize(self.questions)\n",
    "        \n",
    "        print(\"....... VG Question data have been PREPARED & ENCODED ...\")\n",
    "        self.vg_ids = structured_qas['img_id']    \n",
    "        self.question_ids = structured_qas['question_ids']    \n",
    "        self.data_mode = data_mode\n",
    "        \n",
    "        if self.data_mode == 'train':\n",
    "            self.sg_data = TRAIN_SG_DATA\n",
    "        elif data_mode == 'gt_val':\n",
    "            self.sg_data = VAL_GT_SG_DATA\n",
    "        else:\n",
    "            self.sg_data = VAL_INF_SG_DATA\n",
    "            \n",
    "        self.pred_embeddings = load_predicate_features_file()\n",
    "        \n",
    "        print(\"....... Data loading completed ...\")\n",
    "    \n",
    "    @property\n",
    "    def max_que_length(self):\n",
    "        if not hasattr(self, '_ques_max_length'):\n",
    "            self._ques_max_length = max(map(len, self.questions))\n",
    "        return self._ques_max_length\n",
    "    @property\n",
    "    def max_ans_length(self):\n",
    "        if not hasattr(self, '_ans_max_length'):\n",
    "            self._ans_max_length = max(map(len, self.answers))\n",
    "        return self._ans_max_length\n",
    "    \n",
    "    def get_all_img_ftrs(self, img_id):\n",
    "        img_ftr_folder = os.path.join(IMG_FTR_FOLDER, img_id + '.json')\n",
    "        with open(img_ftr_folder, \"r\") as f:\n",
    "            img_ftr_data = json.loads(f.read())\n",
    "            return np.asarray(img_ftr_data, np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.answers)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        img_id = self.vg_ids[item]\n",
    "        qst_id = self.question_ids[item]\n",
    "        \n",
    "        q = self.questions[0][item]\n",
    "        q_length = self.questions[1][item]\n",
    "        a = self.answers[item]\n",
    "        \n",
    "        v = self.get_all_img_ftrs(img_id)\n",
    "    \n",
    "        obj_feat_data_hf = load_object_visual_features(img_id, self.data_mode) \n",
    "        g = self._load_sgraph(img_id, obj_feat_data_hf)\n",
    "            \n",
    "        data = torch_geometric.data.Data(x=g.x.float(), \n",
    "                                         edge_index=g.edge_index.long(),\n",
    "                                         x_sem=g.cat.long(),\n",
    "                                         y=a, edge_attr=g.edge_x.float())\n",
    "        data['v'] = v\n",
    "        data['q'] = q\n",
    "        data['q_length'] = q_length\n",
    "        data['img_id'] = img_id\n",
    "        data['qst_id'] = qst_id\n",
    "        \n",
    "        return data       \n",
    "\n",
    "    \n",
    "    def _load_sgraph(self, image_id, obj_feat_data_hf):\n",
    "        G = nx.DiGraph()\n",
    "        sg_data_per_img = self.sg_data[str(image_id)]\n",
    "\n",
    "        for relation in sg_data_per_img:\n",
    "            sub_id = \"%s\" % (relation[\"subject\"][\"bbox\"])\n",
    "            obj_id = \"%s\" % (relation[\"object\"][\"bbox\"])\n",
    "            pred_id = relation[\"predicate\"] - 1 # -1 is required\n",
    "            sub_cat = relation[\"subject\"][\"category\"]\n",
    "            obj_cat = relation[\"object\"][\"category\"]   \n",
    "            \n",
    "            if G.number_of_nodes() < NUM_NODES - 1:\n",
    "                \n",
    "                G.add_node(sub_id, \n",
    "                           x=np.array(obj_feat_data_hf.get(sub_id , np.zeros(512, ))), \n",
    "                           cat=self.question_field.vocab.stoi[sub_cat])\n",
    "                G.add_node(obj_id, \n",
    "                           x=np.array(obj_feat_data_hf.get(obj_id , np.zeros(512, ))), \n",
    "                           cat=self.question_field.vocab.stoi[obj_cat])         \n",
    "            \n",
    "            if (sub_id in list(G.nodes())) and (obj_id in list(G.nodes())):\n",
    "                \n",
    "                G.add_edge(sub_id, obj_id, edge_x=self.pred_embeddings[pred_id])\n",
    "#                 if pred_id >= 50: # TODO:\n",
    "#                     G.add_edge(sub_id, obj_id, edge_x=np.zeros(300, ) )\n",
    "#                 else:\n",
    "#                     G.add_edge(sub_id, obj_id, edge_x=self.pred_embeddings[pred_id])\n",
    "\n",
    "        g = from_networkx(G)\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "everyday-brand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1 : Data loading\n",
      "....... train mode number of data samples : 897111\n",
      "....... VG Answer data have been PREPARED & ENCODED ...\n",
      "....... VG Question data have been PREPARED & ENCODED ...\n",
      "....... Data loading completed ...\n",
      "Step1 : Data loading\n",
      "....... gt_val mode number of data samples : 125726\n",
      "....... VG Answer data have been PREPARED & ENCODED ...\n",
      "....... VG Question data have been PREPARED & ENCODED ...\n",
      "....... Data loading completed ...\n",
      "897111\n",
      "125726\n"
     ]
    }
   ],
   "source": [
    "train_vqa_dataset = VQA(train_structured_qas,\n",
    "                        QUE_TEXT,\n",
    "                        ANS_TEXT,\n",
    "                        data_mode='train')\n",
    "\n",
    "val_vqa_dataset = VQA(valid_structured_qas,\n",
    "                      QUE_TEXT,\n",
    "                      ANS_TEXT,\n",
    "                      data_mode='gt_val')\n",
    "\n",
    "train_data = train_vqa_dataset.__getitem__(1)\n",
    "print ( train_vqa_dataset.__len__() )\n",
    "\n",
    "print( val_vqa_dataset.__len__() )\n",
    "val_data = val_vqa_dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "judicial-representative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7009\n",
      "983\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_data_loader = torch_geometric.data.DataLoader(train_vqa_dataset,\n",
    "                                                    batch_size=BATCH_SIZE)\n",
    "val_data_loader = torch_geometric.data.DataLoader(val_vqa_dataset,\n",
    "                                                  batch_size=BATCH_SIZE )\n",
    "\n",
    "n_train_batches = len(train_data_loader)\n",
    "print( n_train_batches )\n",
    "\n",
    "n_val_batches = len(val_data_loader)\n",
    "print( n_val_batches )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "civic-crown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in train_data_loader:\n",
    "#     print(x)\n",
    "#     v = torch.tensor(x.v)\n",
    "#     print(v.shape)\n",
    "#     print()\n",
    "#     print(x.img_id[:5])\n",
    "#     print()\n",
    "#     print(x.qst_id[:5])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "diverse-nickel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FOR DEBUGGING\n",
    "# sample_idx = 3\n",
    "# batch_size = 128\n",
    "\n",
    "# for x in train_data_loader:\n",
    "    \n",
    "#     print(x.qst_id)\n",
    "#     print(x.img_id)\n",
    "    \n",
    "#     q = torch.tensor(x.q).view(batch_size, -1)[sample_idx]\n",
    "#     a = torch.tensor(x.y).view(batch_size, -1)[sample_idx]\n",
    "#     print(q.shape)\n",
    "#     print(a.shape)\n",
    "#     print()\n",
    "    \n",
    "#     q_str = []\n",
    "#     for qq in q:\n",
    "#         q_str.append(QUE_TEXT.vocab.itos[qq])\n",
    "#     print(' '.join( q_str) )\n",
    "#     print()\n",
    "    \n",
    "#     a_str = []\n",
    "#     for aa in a:\n",
    "#         a_str.append(ANS_TEXT.vocab.itos[aa])\n",
    "#     print(' '.join( a_str) )\n",
    "#     print()\n",
    "    \n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-ghost",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-oxygen",
   "metadata": {},
   "source": [
    "## Attention-based Sequence Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bulgarian-asthma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.adeveloperdiary.com/data-science/deep-learning/nlp/machine-translation-using-attention-with-pytorch/\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_hidden_dim, decoder_hidden_dim):\n",
    "        super().__init__()\n",
    " \n",
    "        # The input dimension will the the concatenation of\n",
    "        # encoder_hidden_dim (hidden) and  decoder_hidden_dim(encoder_outputs)\n",
    "        self.attn_hidden_vector = nn.Linear(encoder_hidden_dim + decoder_hidden_dim, decoder_hidden_dim)\n",
    " \n",
    "        # We need source len number of values for n batch as the dimension\n",
    "        # of the attention weights. The attn_hidden_vector will have the\n",
    "        # dimension of [source len, batch size, decoder hidden dim]\n",
    "        # If we set the output dim of this Linear layer to 1 then the\n",
    "        # effective output dimension will be [source len, batch size]\n",
    "        self.attn_scoring_fn = nn.Linear(decoder_hidden_dim, 1, bias=False)\n",
    " \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden = [1, batch size, decoder hidden dim]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    " \n",
    "        # We need to calculate the attn_hidden for each source words.\n",
    "        # Instead of repeating this using a loop, we can duplicate\n",
    "        # hidden src_len number of times and perform the operations.\n",
    "        hidden = hidden.repeat(src_len, 1, 1)\n",
    " \n",
    "        # Calculate Attention Hidden values\n",
    "        attn_hidden = torch.tanh(self.attn_hidden_vector(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    " \n",
    "        # Calculate the Scoring function. Remove 3rd dimension.\n",
    "        attn_scoring_vector = self.attn_scoring_fn(attn_hidden).squeeze(2)\n",
    " \n",
    "        # The attn_scoring_vector has dimension of [source len, batch size]\n",
    "        # Since we need to calculate the softmax per record in the batch\n",
    "        # we will switch the dimension to [batch size,source len]\n",
    "        attn_scoring_vector = attn_scoring_vector.permute(1, 0)\n",
    " \n",
    "        # Softmax function for normalizing the weights to\n",
    "        # probability distribution\n",
    "        return F.softmax(attn_scoring_vector, dim=1)\n",
    "    \n",
    "class OneStepDecoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, encoder_hidden_dim, decoder_hidden_dim, dropout_prob=0.5):\n",
    "        super().__init__()\n",
    " \n",
    "        self.attention = Attention(encoder_hidden_dim, decoder_hidden_dim)\n",
    " \n",
    "        # self.embedding = nn.Embedding(input_output_dim, embedding_dim)\n",
    "        self.embedding = nn.Embedding.from_pretrained(ANS_TEXT.vocab.vectors, padding_idx=PAD_token_idx, freeze=True)\n",
    " \n",
    "        # Add the encoder_hidden_dim and embedding_dim\n",
    "        self.rnn = nn.GRU(encoder_hidden_dim + embedding_dim, decoder_hidden_dim)\n",
    "        # Combine all the features for better prediction\n",
    "        self.fc = nn.Linear(encoder_hidden_dim + decoder_hidden_dim + embedding_dim, output_dim)\n",
    " \n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "    \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # Add the source len dimension\n",
    "        input = input.unsqueeze(0)\n",
    " \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    " \n",
    "        # Calculate the attention weights\n",
    "        a = self.attention(hidden, encoder_outputs).unsqueeze(1)\n",
    " \n",
    "        # We need to perform the batch wise dot product.\n",
    "        # Hence need to shift the batch dimension to the front.\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    " \n",
    "        # Use PyTorch's bmm function to calculate the weight W.\n",
    "        W = torch.bmm(a, encoder_outputs)\n",
    " \n",
    "        # Revert the batch dimension.\n",
    "        W = W.permute(1, 0, 2)\n",
    " \n",
    "        # concatenate the previous output with W\n",
    "        rnn_input = torch.cat((embedded, W), dim=2)\n",
    " \n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    " \n",
    "        # Remove the sentence length dimension and pass them to the Linear layer\n",
    "        predicted_token = self.fc(torch.cat((output.squeeze(0), W.squeeze(0), embedded.squeeze(0)), dim=1))\n",
    " \n",
    "        return predicted_token, hidden, a.squeeze(1)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim=300, encoder_hidden_dim=200, decoder_hidden_dim=200, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.one_step_decoder = OneStepDecoder(output_dim, embedding_dim, encoder_hidden_dim, decoder_hidden_dim, dropout_prob)\n",
    "        self.device = device\n",
    "        self.ans_vocab_size = len(ANS_TEXT.vocab)\n",
    " \n",
    "    def forward(self, target, encoder_outputs, hidden, teacher_forcing_ratio=0.5):\n",
    "        batch_size = target.shape[1]\n",
    "        trg_len = target.shape[0]\n",
    "        trg_vocab_size = self.ans_vocab_size\n",
    " \n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size) # .to(self.device)\n",
    "        input = target[0, :]\n",
    " \n",
    "        for t in range(1, trg_len):\n",
    "            # Pass the encoder_outputs. For the first time step the \n",
    "            # hidden state comes from the encoder model.\n",
    "            output, hidden, a = self.one_step_decoder(input, hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    " \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    " \n",
    "            input = target[t] if teacher_force else top1\n",
    " \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-mississippi",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "absent-speech",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes :  1676\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(ANS_TEXT.vocab)\n",
    "print(\"Number of classes : \", n_classes)\n",
    "\n",
    "class GraphGATEncoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout=0.0, heads=1):\n",
    "        super(GraphGATEncoder, self).__init__()\n",
    "        self.gat_conv = GATConv(in_channels=input_dim, out_channels=output_dim, heads=heads) # , dropout=self.dropout)       \n",
    "        \n",
    "    def forward(self, x_input, edge_index):\n",
    "        gat_encoding = self.gat_conv(x_input, edge_index)\n",
    "        return gat_encoding\n",
    "    \n",
    "class SequenceEncoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, \n",
    "                 padding_idx, seq_max_len, \n",
    "                 hidden_size=200, n_layers=1, dropout=0.1):\n",
    "        super(SequenceEncoder, self).__init__()\n",
    "        \n",
    "        self.padding_idx = padding_idx\n",
    "        self.seq_max_len = seq_max_len\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        \n",
    "        self.gru = nn.GRU(input_size, self.hidden_size, self.n_layers,\n",
    "                          batch_first=True, bidirectional=False,\n",
    "                          dropout=(0 if self.n_layers == 1 else self.dropout) )\n",
    "            \n",
    "    def forward(self, seq, seq_len, hidden=None):\n",
    "\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(seq, seq_len, batch_first=True, enforce_sorted=False)\n",
    "        output, final = self.gru(packed, hidden)\n",
    "        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True, padding_value=self.padding_idx, total_length=self.seq_max_len)\n",
    "\n",
    "        return output, final\n",
    "    \n",
    "class VQAEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, q_word_emb_dim, question_length, hidden_size=200, n_layers=1, dropout=0.1):\n",
    "        super(VQAEncoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "                \n",
    "        self.padding_idx = PAD_token_idx\n",
    "        self.q_len = question_length\n",
    "\n",
    "        \n",
    "        \n",
    "        self.q_embedding = nn.Embedding.from_pretrained(QUE_TEXT.vocab.vectors, padding_idx=self.padding_idx, freeze=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.simple_q_encoder = SequenceEncoder(q_word_emb_dim, \n",
    "                                                self.padding_idx, self.q_len, \n",
    "                                                self.hidden_size, self.n_layers, self.dropout)\n",
    "        self.img_q_encoder = SequenceEncoder(q_word_emb_dim + 512, \n",
    "                                             self.padding_idx, self.q_len, \n",
    "                                             self.hidden_size, self.n_layers, self.dropout)\n",
    "\n",
    "        self.g_sem_emb = nn.Embedding.from_pretrained(QUE_TEXT.vocab.vectors, freeze=True)\n",
    "        self.visual_graph_encoder = GraphGATEncoder(input_dim=self.hidden_size+512, output_dim=hidden_size, heads=1)\n",
    "        self.semantic_graph_encoder = GraphGATEncoder(input_dim=self.hidden_size+300, output_dim=hidden_size, heads=1)\n",
    "        \n",
    "        self.pred_fc = nn.Linear(300, hidden_size, bias=False) # [128, 40*40, 300]-->[128, 40*40, 200]\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "                \n",
    "    def forward(self, x, x_sem, edge_index, edge_attr, batch_alloc, qst, qst_len, img, batch_size, encoder_hidden=None):        \n",
    "        qst_embedding_feature = self.q_embedding(qst) # [b, 28, 300]\n",
    "        x_sem_embedding_features = self.g_sem_emb(x_sem) # [N, 300]\n",
    "           \n",
    "        # ================== simple-question encoding ================================== \n",
    "        encoder_outputs, encoder_states = self.simple_q_encoder(qst_embedding_feature, qst_len)                      # [64, 400]\n",
    "        \n",
    "        # ================== question with vgg-img features ========================\n",
    "        vgg_features = torch.unsqueeze(img, 1).repeat(1, self.q_len, 1)\n",
    "        vgg_qst_features = torch.cat((vgg_features, qst_embedding_feature), axis=2) # [128, 14, 812]\n",
    "        enc2_op, qn_encoding2 = self.img_q_encoder(vgg_qst_features, qst_len) # [1, 128, 200]\n",
    "                 \n",
    "        # ================== spread simple-question encoding with node embeddings ======\n",
    "        encoder_states_unsqueezed = torch.squeeze(encoder_states, 0) # [128, 200] \n",
    "        reformatted_batch_qn_encoding = torch.stack([encoder_states_unsqueezed[b_idx] for b_idx in batch_alloc]) # [1605, 200]\n",
    "    \n",
    "        img_ftr_qn_encoding = torch.cat((x, reformatted_batch_qn_encoding), axis=1) # [1605, 200 + 512]\n",
    "        obj_ftr_qn_encoding = torch.cat([x_sem_embedding_features, reformatted_batch_qn_encoding], axis=1) # [1605, 200 + 300]\n",
    "\n",
    "        # ================== apply GAT over simple-question mixed scene-graph ==========\n",
    "        # [1605, 712] --> [1605, 200] --> [128, 40, 200]\n",
    "        img_node_embeddings = self.visual_graph_encoder(img_ftr_qn_encoding, edge_index) # [1605, 200+512] --> [128, 40, 200]\n",
    "        # img_node_embeddings = self.convert_geometric_to_standard_batch(img_node_embeddings, batch_alloc, batch_size)\n",
    "        img_node_embeddings, _ = to_dense_batch(img_node_embeddings, batch=batch_alloc, fill_value=0, max_num_nodes=NUM_NODES)\n",
    "                \n",
    "        obj_node_embeddings = self.semantic_graph_encoder(obj_ftr_qn_encoding, edge_index) # [1605, 200+300] --> [128, 40, 200]\n",
    "        # obj_node_embeddings = self.convert_geometric_to_standard_batch(obj_node_embeddings, batch_alloc, batch_size)\n",
    "        obj_node_embeddings, _ = to_dense_batch(obj_node_embeddings, batch=batch_alloc, fill_value=0, max_num_nodes=NUM_NODES)\n",
    "        \n",
    "        # ================== get predicate embedding =================================== # [744, 300]\n",
    "        tmp_adj = to_dense_adj(edge_index=edge_index, edge_attr=edge_attr, batch=batch_alloc, max_num_nodes=NUM_NODES) # [128, 40, 40, 300]\n",
    "        pred_embeddings = tmp_adj.view(batch_size, NUM_NODES * NUM_NODES, 300) # [128, 40*40, 300]\n",
    "        pred_embeddings_feat = self.tanh( self.pred_fc(pred_embeddings) ) # [128, 40*40, 200]\n",
    "        \n",
    "        # ================== fused both graph-encodings ==================\n",
    "        node_embeddings = torch.cat((img_node_embeddings, pred_embeddings_feat, obj_node_embeddings), \n",
    "                                    axis=1) # [128, 40 + 40*40 + 40, 200]\n",
    "        \n",
    "        return node_embeddings, qn_encoding2 # [128,1680, 200] [1, 128, 200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-maine",
   "metadata": {},
   "source": [
    "## Building full model with Encoder + Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "chemical-frank",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "qst_token_embedding_dim = 300\n",
    "ans_token_embedding_dim = 300\n",
    "\n",
    "HIDDEN_SIZE = 200\n",
    "N_LAYERS = 1\n",
    "DROPOUT = 0.1\n",
    "\n",
    "class VQASeqToSeq(nn.Module):\n",
    "    def __init__(self, qst_token_embedding_dim, ans_token_embedding_dim, hidden_size, n_layers, dropout):\n",
    "        super(VQASeqToSeq, self).__init__()\n",
    "    \n",
    "        self.encoder = VQAEncoder(q_word_emb_dim=qst_token_embedding_dim,\n",
    "                                  question_length=MAX_QUESTION_LENGTH, # CONFIG.MAX_QUESTION_LENGTH + 1,\n",
    "                                  hidden_size=hidden_size, n_layers=n_layers, dropout=dropout)\n",
    "        \n",
    "        self.decoder = Decoder(output_dim=n_classes, \n",
    "                               embedding_dim=ans_token_embedding_dim, \n",
    "                               encoder_hidden_dim=hidden_size, decoder_hidden_dim=hidden_size, dropout_prob=dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, x_sem, edge_index, edge_attr, batch_alloc, qst, qst_len, img, batch_size, \n",
    "                a, teacher_forcing_ratio=0.5, encoder_hidden=None):\n",
    "        node_embeddings, qn_encoding2 = self.encoder(x, x_sem, edge_index, edge_attr, batch_alloc, qst, qst_len, img, batch_size)\n",
    "        node_embeddings = torch.transpose(node_embeddings, 1, 0)\n",
    "        \n",
    "        decoder_output = self.decoder(a, node_embeddings, qn_encoding2, teacher_forcing_ratio)\n",
    "        \n",
    "        return decoder_output\n",
    "    \n",
    "    \n",
    "\n",
    "vqa_seq2seq_model = VQASeqToSeq(qst_token_embedding_dim, ans_token_embedding_dim,\n",
    "                                HIDDEN_SIZE, N_LAYERS, DROPOUT)\n",
    "vqa_seq2seq_model.to(device)\n",
    "print('Models built and ready to go!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-entity",
   "metadata": {},
   "source": [
    "# Define Loss, Optimizer, & utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "accepting-workstation",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, vqa_seq2seq_model.parameters()), \n",
    "                       lr=LEARNING_RATE) #, weight_decay=1e-5)\n",
    "\n",
    "instance_entropy_with_logits = nn.CrossEntropyLoss(ignore_index=PAD_token_idx)\n",
    "\n",
    "def accuracy(pred, true):\n",
    "    acc = (true == pred.argmax(-1)).float().detach().cpu().numpy()\n",
    "    return float(100 * acc.sum() / len(acc))\n",
    "\n",
    "def format_time(elapsed_time):\n",
    "    \"\"\"\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    \"\"\"\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed_time)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded)) # Format as hh:mm:ss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-mapping",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "digital-kitchen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ...\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-41-be19bd83186d>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  q = torch.tensor(batch_data.q).view(batch_size, -1).to(device)\n",
      "<ipython-input-41-be19bd83186d>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  a = torch.transpose( torch.tensor(batch_data.y).view(batch_size, -1), 1, 0).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| TRAIN SET | Epoch [01/03], Step [0000/7009], Loss: 0.7144 , Score: 2.2656 | Elapsed: 0:00:01\n",
      "0:00:01\n",
      "Total training took 0:00:01 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "resume_epoch = 0\n",
    "num_epochs = 3\n",
    "print_every = 10\n",
    "save_every = 1000\n",
    "\n",
    "print('Initializing ...')\n",
    "print(\"Training...\")\n",
    "\n",
    "total_t0 = time.time()\n",
    "for epoch in range(resume_epoch, num_epochs):\n",
    "    total_loss = 0\n",
    "    total_score = 0\n",
    "\n",
    "    vqa_seq2seq_model.train() # IMPORTANT\n",
    "    \n",
    "    t0 = time.time()\n",
    "    for batch_idx, batch_data in enumerate(train_data_loader):\n",
    "        if (batch_idx == n_train_batches-1):\n",
    "            break            \n",
    "            \n",
    "        vqa_seq2seq_model.zero_grad()  # IMPORTANT\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        batch_alloc = batch_data.batch.to(device)\n",
    "        q_len = batch_data.q_length\n",
    "        batch_size = q_len.shape[0]\n",
    "\n",
    "        q = torch.tensor(batch_data.q).view(batch_size, -1).to(device)\n",
    "        a = torch.transpose( torch.tensor(batch_data.y).view(batch_size, -1), 1, 0).to(device)\n",
    "        v = torch.tensor(batch_data.v).to(device)\n",
    "        \n",
    "        edge_attr = batch_data.edge_attr.to(device)\n",
    "        edge_index = batch_data.edge_index.to(device)\n",
    "        x = batch_data.x.to(device)\n",
    "        x_sem = batch_data.x_sem.to(device)\n",
    "\n",
    "        output = vqa_seq2seq_model(x, x_sem, edge_index, edge_attr, batch_alloc, q, q_len, v, batch_size,\n",
    "                     a, 0.5)\n",
    "        output_dim = output.shape[-1]\n",
    "        output_for_loss = output[1:].view(-1, output_dim).to(device)\n",
    "        trg = a[1:].reshape(-1)\n",
    "\n",
    "        output_for_loss_softmaxed = nn.functional.log_softmax(output_for_loss, -1)\n",
    "        loss = instance_entropy_with_logits(output_for_loss_softmaxed, trg) # LOSS\n",
    "\n",
    "        loss.backward()  # IMPORTANT\n",
    "        _ = nn.utils.clip_grad_norm_(vqa_seq2seq_model.parameters(), 0.25)\n",
    "        optimizer.step()  # IMPORTANT\n",
    "        \n",
    "        batch_loss = loss.item()\n",
    "        batch_score = accuracy(output_for_loss, trg) # ACCURACY\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        total_score += batch_score\n",
    "            \n",
    "        with open(os.path.join(SAVE_DIR, 'train-log-epoch.%s.txt' % (SAVE_DIR.split('-')[-1]) ), 'a') as f:\n",
    "                f.write(str(epoch+1) + '\\t' + str(batch_idx+1) + '\\t' + str(batch_loss) + '\\t' + str(batch_score) + '\\n')\n",
    "        writer.add_scalar('training loss', loss.item(), epoch * n_train_batches + batch_idx)\n",
    "        writer.add_scalar('training score', batch_score, epoch * n_train_batches + batch_idx)\n",
    "                \n",
    "        if batch_idx % print_every == 0: # Print progress\n",
    "            total_loss_avg = total_loss / print_every \n",
    "            total_score_avg = total_score / print_every\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('| TRAIN SET | Epoch [{:02d}/{:02d}], Step [{:04d}/{:04d}], Loss: {:.4f} , Score: {:.4f} | Elapsed: {:}'\n",
    "                          .format(epoch+1, num_epochs, batch_idx, int(n_train_batches), total_loss_avg, total_score_avg, elapsed))\n",
    "            total_loss = 0\n",
    "            total_score = 0                \n",
    "        \n",
    "        if ( (batch_idx == n_train_batches-2) or ((batch_idx+1) % save_every == 0) ): # Save checkpoint\n",
    "            directory = os.path.join(SAVE_DIR, 'vqa-pytorch-model')\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save(vqa_seq2seq_model.state_dict(),\n",
    "                       os.path.join(directory, 'epoch-{}.batch-{}.{}.pt'.format(epoch+1, batch_idx+1, 'checkpoint')))\n",
    "\n",
    "training_epoch_time = format_time(time.time() - t0)\n",
    "print(training_epoch_time)\n",
    "        \n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time() - total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-harbor",
   "metadata": {},
   "source": [
    "# Validation on Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "colonial-clinic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_pred_num(true, pred):\n",
    "    correct_pred = (true == pred.argmax(-1)).float().detach().cpu().numpy()\n",
    "    return correct_pred.sum(), len(correct_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "muslim-sigma",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ...\n",
      "Validation ...\n",
      "| VAL SET | Step [0000/0983], Loss: 0.7047 , Score: 1.6406 | Perf: 16.4062 | Elapsed: 0:00:01\n",
      "Final performance ...\n",
      "... final accuracy :  16.40625\n",
      "Total validation took 0:00:01 (h:mm:ss)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-53-cc6ec5d48f2d>:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  q = torch.tensor(batch_data.q).view(batch_size, -1).to(device)\n",
      "<ipython-input-53-cc6ec5d48f2d>:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  a = torch.transpose( torch.tensor(batch_data.y).view(batch_size, -1), 1, 0).to(device)\n"
     ]
    }
   ],
   "source": [
    "val_print_every = 10\n",
    "\n",
    "print('Initializing ...')\n",
    "print(\"Validation ...\")\n",
    "print(\"Validation on ground truth scene-graph\")\n",
    "\n",
    "total_t0 = time.time()\n",
    "total_loss = 0\n",
    "total_score = 0\n",
    "total_num_pred = 0\n",
    "total_num_correct_pred = 0\n",
    "\n",
    "vqa_seq2seq_model.eval() # IMPORTANT\n",
    "\n",
    "prediction_file = os.path.join(SAVE_DIR, \"gt_predictions_%s.json\" % (SAVE_DIR.split('-')[-1]) )\n",
    "prediction_dict = []\n",
    "\n",
    "    \n",
    "t0 = time.time()\n",
    "for batch_idx, batch_data in enumerate(val_data_loader):\n",
    "    if (batch_idx == n_val_batches-1):\n",
    "            break\n",
    "    \n",
    "    batch_alloc = batch_data.batch.to(device)\n",
    "    q_len = batch_data.q_length\n",
    "    batch_size = q_len.shape[0]\n",
    "\n",
    "    q = torch.tensor(batch_data.q).view(batch_size, -1).to(device)\n",
    "    a = torch.transpose( torch.tensor(batch_data.y).view(batch_size, -1), 1, 0).to(device)\n",
    "    v = torch.tensor(batch_data.v).to(device)\n",
    "        \n",
    "    edge_attr = batch_data.edge_attr.to(device)\n",
    "    edge_index = batch_data.edge_index.to(device)\n",
    "    x = batch_data.x.to(device)\n",
    "    x_sem = batch_data.x_sem.to(device)\n",
    "\n",
    "\n",
    "    output = vqa_seq2seq_model(x, x_sem, edge_index, edge_attr, batch_alloc, q, q_len, v, batch_size,\n",
    "                     a, 0.0)\n",
    "    output_dim = output.shape[-1]\n",
    "    output_for_loss = output[1:].view(-1, output_dim).to(device)\n",
    "    trg = a[1:].reshape(-1)\n",
    "        \n",
    "    output_for_loss_softmaxed = nn.functional.log_softmax(output_for_loss, -1)\n",
    "    loss = instance_entropy_with_logits(output_for_loss_softmaxed, trg) # LOSS\n",
    "\n",
    "    batch_loss = loss.item()\n",
    "    batch_score = accuracy(output_for_loss, trg) # ACCURACY\n",
    "    \n",
    "    batch_num_correct_pred, batch_total_pred = correct_pred_num(trg, output_for_loss)\n",
    "        \n",
    "    total_loss += batch_loss\n",
    "    total_score += batch_score\n",
    "    total_num_correct_pred += batch_num_correct_pred\n",
    "    total_num_pred += batch_total_pred\n",
    "\n",
    "    prediction_labels = output_for_loss.argmax(-1)\n",
    "    qst_ids_list = batch_data.qst_id\n",
    "    for sample_idx in range(prediction_labels.shape[0]):\n",
    "        prediction_dict.append( {\"questionId\": qst_ids_list[sample_idx], \n",
    "                                                    \"prediction\": ANS_TEXT.vocab.itos[prediction_labels[sample_idx]]} )\n",
    "\n",
    "    with open(os.path.join(SAVE_DIR, 'gt-val-log-epoch.%s.txt' % (SAVE_DIR.split('-')[-1])), 'a') as f: # Log the loss and accuracy in an epoch.\n",
    "            f.write(str(batch_idx+1) \\\n",
    "                    + '\\t' + str(batch_loss) + '\\t' + str(batch_score) \\\n",
    "                    + '\\t' + str(total_num_correct_pred) + '\\t' + str(total_num_pred) + '\\n')\n",
    "\n",
    "    if batch_idx % val_print_every == 0: # Print progress\n",
    "        total_loss_avg = total_loss / val_print_every \n",
    "        total_score_avg = total_score / val_print_every\n",
    "        perf = float(100 * total_num_correct_pred / total_num_pred)\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "        print('| VAL SET | Step [{:04d}/{:04d}], Loss: {:.4f} , Score: {:.4f} | Perf: {:.4f} | Elapsed: {:}'\n",
    "                .format(batch_idx, int(n_val_batches), total_loss_avg, total_score_avg, perf, elapsed))\n",
    "        total_loss = 0\n",
    "        total_score = 0\n",
    "        \n",
    "final_accuracy = float(100 * total_num_correct_pred / total_num_pred)\n",
    "with open(os.path.join(SAVE_DIR, 'gt-val-log-epoch.%s.txt' % (SAVE_DIR.split('-')[-1])), 'a') as f:\n",
    "    f.write(str(batch_idx+1) + '\\t' + str(batch_loss) + '\\t' + str(batch_score) \\\n",
    "            + '\\t' + str(total_num_correct_pred) + '\\t' + str(total_num_pred) + '\\n')\n",
    "    f.write(\"final accuracy : %s\" % final_accuracy)\n",
    "\n",
    "with open(prediction_file, 'w') as pred_write_file:\n",
    "    json.dump(prediction_dict, pred_write_file, indent=4)\n",
    "    \n",
    "print(\"Final performance on ground truth scene-graph ...\")\n",
    "print(\"... final accuracy : \",  final_accuracy)\n",
    "        \n",
    "print(\"Total validation took {:} (h:mm:ss)\".format(format_time(time.time() - total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-bargain",
   "metadata": {},
   "source": [
    "# Validation on Inferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "written-sport",
   "metadata": {},
   "outputs": [],
   "source": [
    "INF_VAL_ROOT = \"/local/home/rchan31/SIGIR/detailed-experiments/data/INF_VAL_DATA/\"\n",
    "\n",
    "VAL_INF_BBOX_FOLDER = os.path.join(INF_VAL_ROOT, \"inferred_val_bboxes\")\n",
    "VAL_INF_SG_FILE = os.path.join(INF_VAL_ROOT, \"inferred_val_gq_sg_new_sgg.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "competitive-examination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10055\n"
     ]
    }
   ],
   "source": [
    "VAL_INF_SG_DATA = json.load( open(VAL_INF_SG_FILE, \"r\") )\n",
    "print(len(VAL_INF_SG_DATA) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "touched-inventory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading inferred valid_structured_qas ...\n",
      "122213\n"
     ]
    }
   ],
   "source": [
    "def val_inf_get_qas_for_imgs_present(img_id):\n",
    "    if os.path.isfile( os.path.join(IMG_FTR_FOLDER, \"{}.json\".format(img_id)) ) and \\\n",
    "        os.path.isfile( os.path.join(VAL_INF_BBOX_FOLDER, \"{}.json\".format(img_id)) ):\n",
    "        if (img_id in VAL_INF_SG_DATA) and (len(VAL_INF_SG_DATA[str(img_id)]) != 0):\n",
    "            return True\n",
    "    else: False\n",
    "\n",
    "def val_inf_decouple_q_and_a(qas_data):    \n",
    "    structured_qas = {\"img_id\": [], \"questions\": [], \"answers\": [], \"question_ids\": []}\n",
    "    for idx, (qas_id, qas_ins) in enumerate(qas_data.items()):\n",
    "        img_id = qas_ins['imageId']        \n",
    "        \n",
    "        if not (val_inf_get_qas_for_imgs_present(img_id)): continue\n",
    "        \n",
    "        q = qas_ins[\"question\"]\n",
    "        a = qas_ins[\"answer\"]\n",
    "        \n",
    "        structured_qas['img_id'].append(img_id)\n",
    "        structured_qas['questions'].append(q)\n",
    "        structured_qas['answers'].append(a)\n",
    "        structured_qas['question_ids'].append(qas_id)\n",
    "    \n",
    "    return structured_qas\n",
    "\n",
    "val_inf_structured_qas = val_inf_decouple_q_and_a(val_qas_data)\n",
    "print(\"Finished loading inferred valid_structured_qas ...\")\n",
    "\n",
    "print( len(val_inf_structured_qas['answers']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fourth-tsunami",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1 : Data loading\n",
      "....... gt_inf mode number of data samples : 122213\n",
      "....... VG Answer data have been PREPARED & ENCODED ...\n",
      "....... VG Question data have been PREPARED & ENCODED ...\n",
      "....... Data loading completed ...\n",
      "122213\n"
     ]
    }
   ],
   "source": [
    "val_inf_vqa_dataset = VQA(val_inf_structured_qas,\n",
    "                      QUE_TEXT,\n",
    "                      ANS_TEXT,\n",
    "                      data_mode='gt_inf')\n",
    "\n",
    "print( val_inf_vqa_dataset.__len__() )\n",
    "val_data = val_inf_vqa_dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "academic-cutting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "955\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "val_inf_data_loader = torch_geometric.data.DataLoader(val_inf_vqa_dataset,\n",
    "                                                      batch_size=BATCH_SIZE )\n",
    "n_val_inf_batches = len(val_inf_data_loader)\n",
    "print( n_val_inf_batches )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "significant-cabin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ...\n",
      "Validation ...\n",
      "| VAL SET | Step [0000/0983], Loss: 0.7045 , Score: 1.6406 | Perf: 16.4062 | Elapsed: 0:00:01\n",
      "Final performance ...\n",
      "... final accuracy :  16.40625\n",
      "Total validation took 0:00:01 (h:mm:ss)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-57-ce0f1b195c3b>:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  q = torch.tensor(batch_data.q).view(batch_size, -1).to(device)\n",
      "<ipython-input-57-ce0f1b195c3b>:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  a = torch.transpose( torch.tensor(batch_data.y).view(batch_size, -1), 1, 0).to(device)\n"
     ]
    }
   ],
   "source": [
    "val_print_every = 10\n",
    "\n",
    "print('Initializing ...')\n",
    "print(\"Validation ...\")\n",
    "print(\"Validation on inferred scene-graph\")\n",
    "\n",
    "total_t0 = time.time()\n",
    "total_loss = 0\n",
    "total_score = 0\n",
    "total_num_pred = 0\n",
    "total_num_correct_pred = 0\n",
    "\n",
    "vqa_seq2seq_model.eval() # IMPORTANT\n",
    "\n",
    "prediction_file = os.path.join(SAVE_DIR, \"inf_predictions_%s.json\" % (SAVE_DIR.split('-')[-1]) )\n",
    "prediction_dict = []\n",
    "\n",
    "    \n",
    "t0 = time.time()\n",
    "for batch_idx, batch_data in enumerate(val_inf_data_loader):\n",
    "    if (batch_idx == n_val_batches-1):\n",
    "            break\n",
    "    \n",
    "    batch_alloc = batch_data.batch.to(device)\n",
    "    q_len = batch_data.q_length\n",
    "    batch_size = q_len.shape[0]\n",
    "\n",
    "    q = torch.tensor(batch_data.q).view(batch_size, -1).to(device)\n",
    "    a = torch.transpose( torch.tensor(batch_data.y).view(batch_size, -1), 1, 0).to(device)\n",
    "    v = torch.tensor(batch_data.v).to(device)\n",
    "        \n",
    "    edge_attr = batch_data.edge_attr.to(device)\n",
    "    edge_index = batch_data.edge_index.to(device)\n",
    "    x = batch_data.x.to(device)\n",
    "    x_sem = batch_data.x_sem.to(device)\n",
    "\n",
    "\n",
    "    output = vqa_seq2seq_model(x, x_sem, edge_index, edge_attr, batch_alloc, q, q_len, v, batch_size,\n",
    "                     a, 0.0)\n",
    "    output_dim = output.shape[-1]\n",
    "    output_for_loss = output[1:].view(-1, output_dim).to(device)\n",
    "    trg = a[1:].reshape(-1)\n",
    "        \n",
    "    output_for_loss_softmaxed = nn.functional.log_softmax(output_for_loss, -1)\n",
    "    loss = instance_entropy_with_logits(output_for_loss_softmaxed, trg) # LOSS\n",
    "\n",
    "    batch_loss = loss.item()\n",
    "    batch_score = accuracy(output_for_loss, trg) # ACCURACY\n",
    "    \n",
    "    batch_num_correct_pred, batch_total_pred = correct_pred_num(trg, output_for_loss)\n",
    "        \n",
    "    total_loss += batch_loss\n",
    "    total_score += batch_score\n",
    "    total_num_correct_pred += batch_num_correct_pred\n",
    "    total_num_pred += batch_total_pred\n",
    "\n",
    "    prediction_labels = output_for_loss.argmax(-1)\n",
    "    qst_ids_list = batch_data.qst_id\n",
    "    for sample_idx in range(prediction_labels.shape[0]):\n",
    "        prediction_dict.append( {\"questionId\": qst_ids_list[sample_idx], \n",
    "                                                    \"prediction\": ANS_TEXT.vocab.itos[prediction_labels[sample_idx]]} )\n",
    "\n",
    "    with open(os.path.join(SAVE_DIR, 'inf-val-log-epoch.%s.txt' % (SAVE_DIR.split('-')[-1])), 'a') as f: # Log the loss and accuracy in an epoch.\n",
    "            f.write(str(batch_idx+1) \\\n",
    "                    + '\\t' + str(batch_loss) + '\\t' + str(batch_score) \\\n",
    "                    + '\\t' + str(total_num_correct_pred) + '\\t' + str(total_num_pred) + '\\n')\n",
    "\n",
    "    if batch_idx % val_print_every == 0: # Print progress\n",
    "        total_loss_avg = total_loss / val_print_every \n",
    "        total_score_avg = total_score / val_print_every\n",
    "        perf = float(100 * total_num_correct_pred / total_num_pred)\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "        print('| VAL SET | Step [{:04d}/{:04d}], Loss: {:.4f} , Score: {:.4f} | Perf: {:.4f} | Elapsed: {:}'\n",
    "                .format(batch_idx, int(n_val_batches), total_loss_avg, total_score_avg, perf, elapsed))\n",
    "        total_loss = 0\n",
    "        total_score = 0\n",
    "        \n",
    "final_accuracy = float(100 * total_num_correct_pred / total_num_pred)\n",
    "with open(os.path.join(SAVE_DIR, 'inf-val-log-epoch.%s.txt' % (SAVE_DIR.split('-')[-1])), 'a') as f:\n",
    "    f.write(str(batch_idx+1) + '\\t' + str(batch_loss) + '\\t' + str(batch_score) \\\n",
    "            + '\\t' + str(total_num_correct_pred) + '\\t' + str(total_num_pred) + '\\n')\n",
    "    f.write(\"final accuracy : %s\" % final_accuracy)\n",
    "\n",
    "with open(prediction_file, 'w') as pred_write_file:\n",
    "    json.dump(prediction_dict, pred_write_file, indent=4)\n",
    "    \n",
    "print(\"Final performance on inferred scene-graph ...\")\n",
    "print(\"... final accuracy : \",  final_accuracy)\n",
    "        \n",
    "print(\"Total validation took {:} (h:mm:ss)\".format(format_time(time.time() - total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-crisis",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
