{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.utils.data as data\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch.utils\n",
    "import torchtext\n",
    "from torchtext.data import Example\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import h5py\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_SG_FILE = \"/raid6/home/ramraj/SG-VQA/data/scene_graphs.json\"\n",
    "SRC_QA_FILE = \"/raid6/home/ramraj/SG-VQA/data/question_answers.json\"\n",
    "SRC_IMG_DIR = \"/raid6/home/ramraj/SG-VQA/data/VG_100K_ONE/\"\n",
    "\n",
    "IMG_FEATURES_FILE = \"/raid6/home/ramraj/SG-VQA/intermediate-data/one_img_features.h5\"\n",
    "SRC_SPLIT_PATH = \"/raid6/home/ramraj/SG-VQA/data/vg_split.json\"\n",
    "\n",
    "RESULTS_DIR = \"./results_debug_dir/\"\n",
    "SAVE_DIR = \"./saved_debug_dir/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_WORD = '<SOS>'\n",
    "EOS_WORD = '<EOS>'\n",
    "PAD_WORD = '<PAD>'\n",
    "\n",
    "MAX_Q_LEN = 27\n",
    "MAX_A_LEN = 24\n",
    "\n",
    "# ========================= model\n",
    "question_token_embedding_dim = 300\n",
    "attn_model = 'concat' #attn_model = 'general' #attn_model = 'concat' 'dot'\n",
    "hidden_size = 200\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "\n",
    "# ========================= optimizer\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "step_size = 1\n",
    "gamma = 0.001\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "\n",
    "# ========================= training\n",
    "batch_size=32\n",
    "data_worker=1\n",
    "shuffle = True\n",
    "resume_epoch = 0 # saved_model = \"./saved/....\"\n",
    "num_epochs = 1\n",
    "save_step = 1\n",
    "print_every = 50\n",
    "save_every = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108077, 108077)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_data = json.load(open(SRC_SG_FILE, 'r'))\n",
    "qas_data = json.load(open(SRC_QA_FILE, 'r'))\n",
    "len(sg_data), len(qas_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid6/home/ramraj/anaconda3/envs/SG-VQA-gpu-py3/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/raid6/home/ramraj/anaconda3/envs/SG-VQA-gpu-py3/lib/python3.8/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished  0\n",
      "Finished  10000\n",
      "Finished  20000\n",
      "Finished  30000\n",
      "Finished  40000\n",
      "Finished  50000\n",
      "Finished  60000\n",
      "Finished  70000\n",
      "Finished  80000\n",
      "Finished  90000\n",
      "Finished  100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../cache/glove.6B.zip: 862MB [06:30, 2.21MB/s]                               \n",
      "100%|█████████▉| 399999/400000 [00:59<00:00, 6684.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Question ======\n",
      "torch.Size([23335, 300])\n",
      "======= Answer ========\n",
      "torch.Size([18046, 300])\n"
     ]
    }
   ],
   "source": [
    "r\"\"\"\n",
    "generated\n",
    "1. question fixed length (including EOS) = MAX_Q_LEN + 1\n",
    "2. answer fixed length (no EOS) = MAX_A_LEN\n",
    "\"\"\"\n",
    "\n",
    "# 1. Define the fields\n",
    "QUE_TEXT = torchtext.data.Field(sequential=True, \n",
    "                                tokenize=lambda x: x.split(),\n",
    "                                # init_token=SOS_WORD,\n",
    "                                eos_token=EOS_WORD,\n",
    "                                pad_token=PAD_WORD,\n",
    "                                include_lengths=True,\n",
    "                                batch_first=True,\n",
    "                                fix_length=MAX_Q_LEN + 1, # ['who', 'are', 'you', 'EOS'] ==> +1 for 'EOS'\n",
    "                                lower=True) # todo: do i need init_token, eos_token, preprocessing ?\n",
    "\n",
    "ANS_TEXT = torchtext.data.Field(sequential=True, \n",
    "                                tokenize=lambda x: x.split(),                                \n",
    "                                # eos_token=EOS_WORD, # init_token=SOS_WORD,\n",
    "                                pad_token=PAD_WORD,\n",
    "                                batch_first=True,\n",
    "                                fix_length=MAX_A_LEN, # ['SOS', i', 'am', 'a', 'pilot']\n",
    "                                lower=True) # todo: do i need init_token, eos_token, preprocessing ?\n",
    "\n",
    "\n",
    "FIELDS = [('ans_text', ANS_TEXT), ('que_text', QUE_TEXT)]\n",
    "\n",
    "example_texts = []\n",
    "\n",
    "for idx, qas_ins in enumerate(qas_data):\n",
    "\n",
    "    for qa_idx, qa in enumerate(qas_ins['qas']):\n",
    "        a = qa['answer']\n",
    "        a = re.sub(r'[^\\w\\s]', ' ', a).lower().strip()        \n",
    "        \n",
    "        q = qa['question']\n",
    "        q = re.sub(r'[^\\w\\s]', ' ', q).lower().strip()        \n",
    "        \n",
    "        example_texts.append( Example.fromlist([a, q] , FIELDS ) )\n",
    "\n",
    "    if idx % 10000 == 0:\n",
    "        print(\"Finished \", idx)\n",
    "        \n",
    "torchtext_dataset = torchtext.data.Dataset(example_texts, fields=FIELDS)\n",
    "\n",
    "ANS_TEXT.build_vocab(torchtext_dataset, vectors='glove.6B.300d', vectors_cache='../cache')\n",
    "QUE_TEXT.build_vocab(torchtext_dataset, vectors='glove.6B.300d', vectors_cache='../cache')\n",
    "\n",
    "print(\"======= Question ======\")\n",
    "print(ANS_TEXT.vocab.vectors.size())\n",
    "\n",
    "print(\"======= Answer ========\")\n",
    "print(QUE_TEXT.vocab.vectors.size())\n",
    "\n",
    "PAD_token = QUE_TEXT.vocab.stoi[PAD_WORD]\n",
    "SOS_token = QUE_TEXT.vocab.stoi[SOS_WORD]\n",
    "EOS_token = QUE_TEXT.vocab.stoi[EOS_WORD]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading train_structured_qas ...\n",
      "Finished loading valid_structured_qas ...\n"
     ]
    }
   ],
   "source": [
    "def decouple_q_and_a(qas_data, mode_id_list):\n",
    "    structured_qas = {\"qas_id\": [], \"qa_id\": [], \"questions\": [], \"answers\": []}\n",
    "    for qas_idx, qas in enumerate(qas_data):\n",
    "        if not qas['id'] in mode_id_list:\n",
    "            continue\n",
    "        for qa in qas['qas']:\n",
    "            structured_qas['qas_id'].append(qas['id'])\n",
    "            structured_qas['qa_id'].append(qa['qa_id'])\n",
    "            structured_qas['questions'].append(qa['question'])\n",
    "            structured_qas['answers'].append(qa['answer'])\n",
    "    \n",
    "    return structured_qas\n",
    "\n",
    "\n",
    "with open(SRC_SPLIT_PATH, 'r') as split_df:\n",
    "    data_ids = json.load(split_df)\n",
    "    train_data_ids = data_ids['train']\n",
    "    valid_data_ids = data_ids['val']\n",
    "\n",
    "train_structured_qas = decouple_q_and_a(qas_data, train_data_ids)\n",
    "print(\"Finished loading train_structured_qas ...\")\n",
    "valid_structured_qas = decouple_q_and_a(qas_data, valid_data_ids)\n",
    "print(\"Finished loading valid_structured_qas ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = QUE_TEXT.vocab.stoi[PAD_WORD]\n",
    "SOS_token = QUE_TEXT.vocab.stoi[SOS_WORD]\n",
    "EOS_token = QUE_TEXT.vocab.stoi[EOS_WORD]\n",
    "\n",
    "def prepare_questions(questions_list):\n",
    "    for q in questions_list:\n",
    "        q = re.sub(r'[^\\w\\s]', ' ', q).lower().strip()\n",
    "        yield q.split()\n",
    "\n",
    "def prepare_answers(answers_list):\n",
    "    for a in answers_list:\n",
    "        a = re.sub(r'[^\\w\\s]', ' ', a).lower().strip()\n",
    "        yield a.split()\n",
    "        \n",
    "class VQA(torch.utils.data.Dataset):\n",
    "    \"\"\" VQA dataset, open-ended \"\"\"\n",
    "    def __init__(self,\n",
    "                image_features_path,\n",
    "                structured_qas,\n",
    "                question_field,\n",
    "                answer_field,\n",
    "                data_mode=\"train\"):\n",
    "        super(VQA, self).__init__()\n",
    "\n",
    "        print(\"Step1 : Data loading\")\n",
    "        print(\"....... {} mode number of data samples : {}\".format(data_mode, len(structured_qas['qas_id'])))\n",
    "\n",
    "        # ========================= Load Answer Vocab =================\n",
    "        self.answers_vocab = answer_field.vocab\n",
    "        self.answers = list(prepare_answers( structured_qas['answers'] ))\n",
    "        self.answers = answer_field.pad(self.answers)\n",
    "        # print(self.answers[:2])\n",
    "        self.answers = answer_field.numericalize(self.answers)\n",
    "        # print(self.answers[:2])\n",
    "        print(\"....... VG Answer data have been PREPARED & ENCODED ...\")\n",
    "\n",
    "        # ========================= Load Question Vocab =================\n",
    "        self.questions_vocab = question_field.vocab\n",
    "        self.questions = list(prepare_questions( structured_qas['questions'] ))\n",
    "        self.questions = question_field.pad(self.questions)\n",
    "        # print(self.questions[0][:2])\n",
    "        # print(self.questions[1][:2])\n",
    "        self.questions = question_field.numericalize(self.questions)\n",
    "        # print(self.questions[0][:2])\n",
    "        # print(self.questions[1][:2])\n",
    "        print(\"....... VG Question data have been PREPARED & ENCODED ...\")\n",
    "    \n",
    "        self.image_features_path = image_features_path\n",
    "        self.features_file = h5py.File(self.image_features_path, 'r') \n",
    "        self.vg_ids = structured_qas['qas_id']\n",
    "        print(\"....... Done Data loading ...\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        assert len(self.answers) != len(self.questions), \"mismatched questions & answer sample size\"\n",
    "        return len(self.answers)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        q = self.questions[0][item]\n",
    "        q_length = self.questions[1][item]\n",
    "        a = self.answers[item]\n",
    "        \n",
    "        img_id = self.vg_ids[item]\n",
    "        v = self._load_image(img_id)\n",
    "\n",
    "        mask = self._binaryMatrix(q)        \n",
    "        mask = torch.BoolTensor(mask)\n",
    "        \n",
    "        return v, q, a, mask, q_length, item\n",
    "    \n",
    "    def _binaryMatrix(self, seq, value=PAD_token):\n",
    "        m = []\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m.append(0)\n",
    "            else:\n",
    "                m.append(1)\n",
    "        return m\n",
    "\n",
    "    def _encode_answers(self, answer):\n",
    "        answer_return = [self.answers_vocab.stoi[SOS_WORD]] # SOS token\n",
    "        for a in answer:\n",
    "            answer_return.append(self.answers_vocab.stoi[a])\n",
    "        answer_return.append(self.answers_vocab.stoi[EOS_WORD]) # EOS token\n",
    "        return answer_return\n",
    "    \n",
    "    def _encode_questions(self, question):\n",
    "        question_return = [self.questions_vocab.stoi[SOS_WORD]] # SOS token\n",
    "        for q in question:\n",
    "            question_return.append(self.questions_vocab.stoi[q])\n",
    "        question_return.append(self.questions_vocab.stoi[EOS_WORD]) # EOS token\n",
    "        return question_return, len(question) + 2\n",
    "    \n",
    "    @property\n",
    "    def max_que_length(self):\n",
    "        if not hasattr(self, '_ques_max_length'):\n",
    "            self._ques_max_length = max(map(len, self.questions))\n",
    "        return self._ques_max_length\n",
    "    @property\n",
    "    def max_ans_length(self):\n",
    "        if not hasattr(self, '_ans_max_length'):\n",
    "            self._ans_max_length = max(map(len, self.answers))\n",
    "        return self._ans_max_length\n",
    "\n",
    "\n",
    "    def _load_image(self, img_id):\n",
    "        if not hasattr(self, 'features_file'):\n",
    "            self.features_file = h5py.File(self.image_features_path, 'r')\n",
    "        img = np.array(self.features_file.get( str(img_id)) )\n",
    "        return torch.from_numpy(img).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1 : Data loading\n",
      "....... train mode number of data samples : 1156933\n",
      "....... VG Answer data have been PREPARED & ENCODED ...\n",
      "....... VG Question data have been PREPARED & ENCODED ...\n",
      "....... Done Data loading ...\n",
      "Step1 : Data loading\n",
      "....... val mode number of data samples : 1156933\n",
      "....... VG Answer data have been PREPARED & ENCODED ...\n",
      "....... VG Question data have been PREPARED & ENCODED ...\n",
      "....... Done Data loading ...\n"
     ]
    }
   ],
   "source": [
    "train_vqa_dataset = VQA(IMG_FEATURES_FILE,\n",
    "                        train_structured_qas,\n",
    "                        QUE_TEXT,\n",
    "                        ANS_TEXT,\n",
    "                        data_mode='train')\n",
    "\n",
    "valid_vqa_dataset = VQA(IMG_FEATURES_FILE,\n",
    "                        train_structured_qas,\n",
    "                        QUE_TEXT,\n",
    "                        ANS_TEXT,\n",
    "                        data_mode='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36154 36154\n",
      "1156933 1156933\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    # put question lengths in descending order so that we can use packed sequences later\n",
    "    batch.sort(key=lambda x: x[-2], reverse=True)\n",
    "    return data.dataloader.default_collate(batch)\n",
    "\n",
    "\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(dataset=train_vqa_dataset,\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=shuffle,\n",
    "                                                collate_fn=collate_fn,\n",
    "                                                drop_last=True,\n",
    "                                                num_workers=data_worker)\n",
    "valid_data_loader = torch.utils.data.DataLoader(dataset=valid_vqa_dataset,\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=shuffle,\n",
    "                                                collate_fn=collate_fn,\n",
    "                                                drop_last=True,\n",
    "                                                num_workers=data_worker)\n",
    "\n",
    "print(len(train_data_loader), len(valid_data_loader))\n",
    "print(train_vqa_dataset.__len__(), valid_vqa_dataset.__len__())\n",
    "\n",
    "n_train_batches = len(train_data_loader)\n",
    "n_val_batches = len(valid_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, q_word_emb_dim, hidden_size, q_embedding, question_length, padding_idx, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.q_len = question_length\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = q_embedding\n",
    "        self.gru = nn.GRU(q_word_emb_dim, hidden_size, n_layers,  # todo: what is input size ?\n",
    "                          dropout=(0 if n_layers == 1 else dropout), \n",
    "                          bidirectional=True)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.img_encoder_lin = nn.Linear(512, hidden_size, bias=True) # TODO: replace with constant\n",
    "\n",
    "    def forward(self, img, input_seq, input_lengths, hidden=None): \n",
    "        \n",
    "        embedded = self.embedding(input_seq)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths, batch_first=True)        \n",
    "        outputs, hidden = self.gru(packed, hidden) # output: (seq_len, batch, hidden*n_dir)        \n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs,\n",
    "                                                            batch_first=False, \n",
    "                                                            padding_value=self.padding_idx,\n",
    "                                                            total_length=self.q_len)\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Sum bidirectional outputs (1, batch, hidden)\n",
    "        \n",
    "        hidden = torch.mul(hidden, self.img_encoder_lin(img))\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Calculate the attention weights (energies) based on the given method\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "        # Transpose max_length and batch_size dimensions\n",
    "        attn_energies = attn_energies.t()\n",
    "\n",
    "        # Return the softmax normalized probability scores (with added dimension)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, q_word_emb_dim, attn_model, a_embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = a_embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(q_word_emb_dim, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        # Choose attention model\n",
    "        if attn_model != 'none':\n",
    "            self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        embedded = self.embedding(input_seq)\n",
    "        embedded = self.embedding_dropout(embedded) #[1, 64, 512]\n",
    "        if(embedded.size(0) != 1):\n",
    "            raise ValueError('Decoder input sequence length should be 1')\n",
    "\n",
    "        # Get current hidden state from input word and last hidden state\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "\n",
    "        # Calculate attention from current RNN state and all encoder outputs;\n",
    "        # apply to encoder outputs to get weighted average\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs) #[64, 1, 14]\n",
    "        # encoder_outputs [14, 64, 512]\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) #[64, 1, 512]\n",
    "\n",
    "        # Attentional vector using the RNN hidden state and context vector\n",
    "        # concatenated together (Luong eq. 5)\n",
    "        rnn_output = rnn_output.squeeze(0) #[64, 512]\n",
    "        context = context.squeeze(1) #[64, 512]\n",
    "        concat_input = torch.cat((rnn_output, context), 1) #[64, 1024]\n",
    "        concat_output = torch.tanh(self.concat(concat_input)) #[64, 512]\n",
    "\n",
    "        # # Finally predict next token (Luong eq. 6, without softmax)\n",
    "        # output = self.out(concat_output) #[64, output_size]\n",
    "\n",
    "        # # Return final output, hidden state, and attention weights (for visualization)\n",
    "        # return output, hidden, attn_weights\n",
    "\n",
    "        # Predict next word using Luong eq. 6\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        # Return output and final hidden state\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()\n",
    "\n",
    "def format_time(elapsed_time):\n",
    "    \"\"\"\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    \"\"\"\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed_time)))\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "question_embedding = nn.Embedding.from_pretrained(QUE_TEXT.vocab.vectors, padding_idx=PAD_token, freeze=False)\n",
    "answer_embedding = nn.Embedding.from_pretrained(ANS_TEXT.vocab.vectors, padding_idx=PAD_token, freeze=False)\n",
    "\n",
    "encoder = EncoderRNN(q_word_emb_dim=question_token_embedding_dim,\n",
    "                     hidden_size=hidden_size,\n",
    "                     q_embedding=question_embedding,\n",
    "                     question_length=MAX_Q_LEN + 1, # FIXED LEN OF QUESTION\n",
    "                     padding_idx=PAD_token,\n",
    "                     n_layers=encoder_n_layers,\n",
    "                     dropout=dropout)\n",
    "\n",
    "decoder = LuongAttnDecoderRNN(q_word_emb_dim=question_token_embedding_dim,\n",
    "                              attn_model=attn_model,\n",
    "                              a_embedding=answer_embedding,\n",
    "                              hidden_size=hidden_size,\n",
    "                              output_size=len(ANS_TEXT.vocab),\n",
    "                              n_layers=decoder_n_layers,\n",
    "                              dropout=dropout)\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print('Models built and ready to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_default = 1e-3 # if eval_loader is not None else 7e-4\n",
    "# lr_decay_step = 2\n",
    "# lr_decay_rate = .25\n",
    "# lr_decay_epochs = range(10,20,lr_decay_step) # if eval_loader is not None else range(10,20,lr_decay_step)\n",
    "# gradual_warmup_steps = [0.5 * lr_default, 1.0 * lr_default, 1.5 * lr_default, 2.0 * lr_default]\n",
    "# # saving_epoch = 3\n",
    "# grad_clip = .25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building optimizers ...\n"
     ]
    }
   ],
   "source": [
    "lr_default = 0.0001\n",
    "\n",
    "# Ensure dropout layers are in train mode\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "print('Building optimizers ...') # Initialize optimizers\n",
    "# encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate * decoder_learning_ratio) # remove '* decoder_learning_ratio'\n",
    "# decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "encoder_optimizer = optim.Adamax(filter(lambda p: p.requires_grad, encoder.parameters()), lr=lr_default)\n",
    "decoder_optimizer = optim.Adamax(filter(lambda p: p.requires_grad, decoder.parameters()), lr=lr_default)\n",
    "    \n",
    "# If you have cuda, configure cuda to call\n",
    "for state in encoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "for state in decoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(v, q, a, mask, q_len,\n",
    "          max_target_len, max_question_len,\n",
    "          encoder, decoder,\n",
    "          encoder_optimizer, decoder_optimizer, batch_size, clip):\n",
    "    \n",
    "    target_variable = a.view(max_target_len, -1)\n",
    "    mask = mask.view(max_question_len, -1)\n",
    "\n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Set device options\n",
    "    v, q, target_variable, mask = v.to(device), q.to(device), target_variable.to(device), mask.to(device)\n",
    "    # Lengths for rnn packing should always be on the cpu\n",
    "    q_len = q_len.to(\"cpu\")\n",
    "\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    # print(q_len)\n",
    "    encoder_outputs, encoder_hidden = encoder(v, q, q_len)\n",
    "\n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "    # Determine if we are using teacher forcing this iteration\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Teacher forcing: next input is current target\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # No teacher forcing: next input is decoder's own current output\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    # Perform backpropatation\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training!\n",
      "Initializing ...\n",
      "Training...\n",
      "| TRAIN SET | Epoch [01/01], Step [0000/36154], Loss: 0.1996 | Elapsed: 0:00:01\n",
      "| TRAIN SET | Epoch [01/01], Step [0050/36154], Loss: 9.9691 | Elapsed: 0:00:13\n",
      "| TRAIN SET | Epoch [01/01], Step [0100/36154], Loss: 9.9692 | Elapsed: 0:00:25\n",
      "| TRAIN SET | Epoch [01/01], Step [0150/36154], Loss: 9.9696 | Elapsed: 0:00:38\n",
      "| TRAIN SET | Epoch [01/01], Step [0200/36154], Loss: 9.9709 | Elapsed: 0:00:50\n",
      "| TRAIN SET | Epoch [01/01], Step [0250/36154], Loss: 9.9709 | Elapsed: 0:01:03\n",
      "| TRAIN SET | Epoch [01/01], Step [0300/36154], Loss: 9.9679 | Elapsed: 0:01:15\n",
      "| TRAIN SET | Epoch [01/01], Step [0350/36154], Loss: 9.9677 | Elapsed: 0:01:27\n",
      "| TRAIN SET | Epoch [01/01], Step [0400/36154], Loss: 9.9698 | Elapsed: 0:01:40\n",
      "| TRAIN SET | Epoch [01/01], Step [0450/36154], Loss: 9.9679 | Elapsed: 0:01:52\n",
      "| TRAIN SET | Epoch [01/01], Step [0500/36154], Loss: 9.9686 | Elapsed: 0:02:04\n",
      "| TRAIN SET | Epoch [01/01], Step [0550/36154], Loss: 9.9713 | Elapsed: 0:02:16\n",
      "| TRAIN SET | Epoch [01/01], Step [0600/36154], Loss: 9.9695 | Elapsed: 0:02:28\n",
      "| TRAIN SET | Epoch [01/01], Step [0650/36154], Loss: 9.9689 | Elapsed: 0:02:39\n",
      "| TRAIN SET | Epoch [01/01], Step [0700/36154], Loss: 9.9721 | Elapsed: 0:02:51\n",
      "| TRAIN SET | Epoch [01/01], Step [0750/36154], Loss: 9.9699 | Elapsed: 0:03:04\n",
      "| TRAIN SET | Epoch [01/01], Step [0800/36154], Loss: 9.9698 | Elapsed: 0:03:16\n",
      "| TRAIN SET | Epoch [01/01], Step [0850/36154], Loss: 9.9707 | Elapsed: 0:03:28\n",
      "| TRAIN SET | Epoch [01/01], Step [0900/36154], Loss: 9.9684 | Elapsed: 0:03:40\n",
      "| TRAIN SET | Epoch [01/01], Step [0950/36154], Loss: 9.9708 | Elapsed: 0:03:52\n",
      "| TRAIN SET | Epoch [01/01], Step [1000/36154], Loss: 9.9704 | Elapsed: 0:04:04\n",
      "| TRAIN SET | Epoch [01/01], Step [1050/36154], Loss: 9.9714 | Elapsed: 0:04:16\n",
      "| TRAIN SET | Epoch [01/01], Step [1100/36154], Loss: 9.9676 | Elapsed: 0:04:28\n",
      "| TRAIN SET | Epoch [01/01], Step [1150/36154], Loss: 9.9711 | Elapsed: 0:04:39\n",
      "| TRAIN SET | Epoch [01/01], Step [1200/36154], Loss: 9.9708 | Elapsed: 0:04:51\n",
      "| TRAIN SET | Epoch [01/01], Step [1250/36154], Loss: 9.9706 | Elapsed: 0:05:01\n",
      "| TRAIN SET | Epoch [01/01], Step [1300/36154], Loss: 9.9715 | Elapsed: 0:05:12\n",
      "| TRAIN SET | Epoch [01/01], Step [1350/36154], Loss: 9.9704 | Elapsed: 0:05:23\n",
      "| TRAIN SET | Epoch [01/01], Step [1400/36154], Loss: 9.9692 | Elapsed: 0:05:34\n",
      "| TRAIN SET | Epoch [01/01], Step [1450/36154], Loss: 9.9685 | Elapsed: 0:05:45\n",
      "| TRAIN SET | Epoch [01/01], Step [1500/36154], Loss: 9.9696 | Elapsed: 0:05:56\n",
      "| TRAIN SET | Epoch [01/01], Step [1550/36154], Loss: 9.9701 | Elapsed: 0:06:07\n",
      "| TRAIN SET | Epoch [01/01], Step [1600/36154], Loss: 9.9720 | Elapsed: 0:06:19\n",
      "| TRAIN SET | Epoch [01/01], Step [1650/36154], Loss: 9.9695 | Elapsed: 0:06:31\n",
      "| TRAIN SET | Epoch [01/01], Step [1700/36154], Loss: 9.9694 | Elapsed: 0:06:42\n",
      "| TRAIN SET | Epoch [01/01], Step [1750/36154], Loss: 9.9688 | Elapsed: 0:06:53\n",
      "| TRAIN SET | Epoch [01/01], Step [1800/36154], Loss: 9.9691 | Elapsed: 0:07:05\n",
      "| TRAIN SET | Epoch [01/01], Step [1850/36154], Loss: 9.9705 | Elapsed: 0:07:16\n",
      "| TRAIN SET | Epoch [01/01], Step [1900/36154], Loss: 9.9695 | Elapsed: 0:07:27\n",
      "| TRAIN SET | Epoch [01/01], Step [1950/36154], Loss: 9.9694 | Elapsed: 0:07:38\n",
      "| TRAIN SET | Epoch [01/01], Step [2000/36154], Loss: 9.9695 | Elapsed: 0:07:50\n",
      "| TRAIN SET | Epoch [01/01], Step [2050/36154], Loss: 9.9694 | Elapsed: 0:08:00\n",
      "| TRAIN SET | Epoch [01/01], Step [2100/36154], Loss: 9.9681 | Elapsed: 0:08:11\n",
      "| TRAIN SET | Epoch [01/01], Step [2150/36154], Loss: 9.9704 | Elapsed: 0:08:22\n",
      "| TRAIN SET | Epoch [01/01], Step [2200/36154], Loss: 9.9697 | Elapsed: 0:08:34\n",
      "| TRAIN SET | Epoch [01/01], Step [2250/36154], Loss: 9.9694 | Elapsed: 0:08:45\n",
      "| TRAIN SET | Epoch [01/01], Step [2300/36154], Loss: 9.9694 | Elapsed: 0:08:56\n",
      "| TRAIN SET | Epoch [01/01], Step [2350/36154], Loss: 9.9709 | Elapsed: 0:09:07\n",
      "| TRAIN SET | Epoch [01/01], Step [2400/36154], Loss: 9.9684 | Elapsed: 0:09:19\n",
      "| TRAIN SET | Epoch [01/01], Step [2450/36154], Loss: 9.9695 | Elapsed: 0:09:30\n",
      "| TRAIN SET | Epoch [01/01], Step [2500/36154], Loss: 9.9699 | Elapsed: 0:09:41\n"
     ]
    }
   ],
   "source": [
    "import logger\n",
    "\n",
    "def instance_bce_with_logits(logits, labels, reduction='mean'):\n",
    "    assert logits.dim() == 2\n",
    "\n",
    "    loss = nn.functional.binary_cross_entropy_with_logits(logits, labels, reduction=reduction)\n",
    "    if reduction == 'mean':\n",
    "        loss *= labels.size(1)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def trainIters(model_name, encoder, decoder, encoder_optimizer, decoder_optimizer, \n",
    "               encoder_n_layers, decoder_n_layers, save_dir, resume_epoch, num_epochs, batch_size, \n",
    "               print_every, save_every, clip, loadFilename):\n",
    "\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "\n",
    "    print(\"Training...\")\n",
    "    total_t0 = time.time()\n",
    "    for epoch in range(resume_epoch, num_epochs):\n",
    "        \n",
    "        total_loss = 0\n",
    "        train_score = 0\n",
    "        total_norm = 0\n",
    "        count_norm = 0\n",
    "        \n",
    "#         if epoch < len(gradual_warmup_steps):\n",
    "#             optim.param_groups[0]['lr'] = gradual_warmup_steps[epoch]\n",
    "#             logger.write('gradual warmup lr: %.4f' % optim.param_groups[0]['lr'])\n",
    "#         elif epoch in lr_decay_epochs:\n",
    "#             optim.param_groups[0]['lr'] *= lr_decay_rate\n",
    "#             logger.write('decreased lr: %.4f' % optim.param_groups[0]['lr'])\n",
    "#         else:\n",
    "#             logger.write('lr: %.4f' % optim.param_groups[0]['lr'])\n",
    "            \n",
    "        \n",
    "        running_loss = 0.0\n",
    "        running_corr_exp1 = 0\n",
    "        t0 = time.time()\n",
    "        for batch_idx, batch in enumerate(train_data_loader):\n",
    "            v, q, a, mask, q_len, _ = batch\n",
    "        \n",
    "            # Run a training iteration with batch\n",
    "            loss = train(v, q, a, mask, q_len,\n",
    "                         MAX_A_LEN, MAX_Q_LEN + 1, # FIXED Q & A LEN\n",
    "                         encoder, decoder,\n",
    "                         encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "            print_loss += loss\n",
    "        \n",
    "\n",
    "            # Print progress\n",
    "            if batch_idx % print_every == 0:\n",
    "                print_loss_avg = print_loss / print_every\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                # print(\"Batch: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(batch_idx, batch_idx / n_train_batches * 100, print_loss_avg))\n",
    "                print('| TRAIN SET | Epoch [{:02d}/{:02d}], Step [{:04d}/{:04d}], Loss: {:.4f} | Elapsed: {:}'\n",
    "                          .format(epoch+1, num_epochs, batch_idx, int(n_train_batches), print_loss_avg, elapsed))\n",
    "                print_loss = 0\n",
    "                \n",
    "            # Log the loss and accuracy in an epoch.\n",
    "            with open(os.path.join(SAVE_DIR, 'train-log-epoch.txt'), 'a') as f:\n",
    "                f.write(str(epoch+1) + '\\t'\n",
    "                        + str(print_loss) + '\\t'\n",
    "                        + str(print_loss_avg))\n",
    "                \n",
    "            # break # TODO:\n",
    "\n",
    "        # Save checkpoint\n",
    "        if (epoch % save_every == 0):\n",
    "            directory = os.path.join(save_dir, model_name, 'final-{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            # TODO: change above name 'remove final term'\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'question_embedding': question_embedding.state_dict(),\n",
    "                'answer_embedding': answer_embedding.state_dict(),\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(epoch, 'checkpoint')))\n",
    "            \n",
    "        training_epoch_time = format_time(time.time() - t0)\n",
    "        print(training_epoch_time)\n",
    "        \n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time() - total_t0)))\n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "# Run training iterations\n",
    "print(\"Starting Training!\")\n",
    "\n",
    "model_name = 'cb_model'\n",
    "loadFilename = False # todo\n",
    "\n",
    "trainIters(model_name, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           encoder_n_layers, decoder_n_layers, SAVE_DIR, resume_epoch, num_epochs, batch_size,\n",
    "           print_every, save_every, clip, loadFilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def correct_tokens(pred, true_tokens, padding_idx):\n",
    "#     pred = pred.view(-1)\n",
    "#     # true_tokens = true_tokens[:, 1:].contiguous()\n",
    "#     non_padding = true_tokens.view(-1).ne(padding_idx)\n",
    "#     num_correct = pred.eq(true_tokens.view(-1)).masked_select(non_padding).sum().item()\n",
    "#     num_non_padding = non_padding.sum().item()\n",
    "#     return num_non_padding, num_correct\n",
    "\n",
    "\n",
    "# def eval(v, q, a, mask, q_len,\n",
    "#           max_target_len, max_question_len,\n",
    "#           encoder, decoder, batch_size):\n",
    "\n",
    "#     target_variable = a.view(max_target_len, -1)\n",
    "#     mask = mask.view(max_question_len, -1)\n",
    "\n",
    "#     # Set device options\n",
    "#     v, q, target_variable, mask = v.to(device), q.to(device), target_variable.to(device), mask.to(device)\n",
    "#     # Lengths for rnn packing should always be on the cpu\n",
    "#     q_len = q_len.to(\"cpu\")\n",
    "\n",
    "#     # Initialize variables\n",
    "#     loss = 0\n",
    "#     print_losses = []\n",
    "#     n_totals = 0\n",
    "    \n",
    "#     all_tokens = torch.zeros([0], device=device, dtype=torch.long) # TODO: # Initialize tensors to append decoded words to\n",
    "#     all_scores = torch.zeros([0], device=device)\n",
    "\n",
    "#     # Forward pass through encoder\n",
    "#     encoder_outputs, encoder_hidden = encoder(v, q, q_len)\n",
    "\n",
    "#     # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "#     decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "#     decoder_input = decoder_input.to(device)\n",
    "\n",
    "\n",
    "#     # Set initial decoder hidden state to the encoder's final hidden state\n",
    "#     decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "#     for t in range(max_target_len):\n",
    "#         decoder_output, decoder_hidden = decoder(\n",
    "#             decoder_input, decoder_hidden, encoder_outputs\n",
    "#         )\n",
    "#         # No teacher forcing: next input is decoder's own current output\n",
    "#         # _, topi = decoder_output.topk(1)\n",
    "#         # decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "#         # decoder_input = decoder_input.to(device)\n",
    "        \n",
    "#         # other code block\n",
    "#         decoder_scores, decoder_input = torch.max(decoder_output, dim=1) # Obtain most likely word token and its softmax score\n",
    "#         # print(decoder_scores.shape, decoder_input.shape)\n",
    "#         decoder_input = torch.unsqueeze(decoder_input, 0) # Prepare current token to be next decoder input (add a dimension)\n",
    "#         decoder_scores = torch.unsqueeze(decoder_scores, 0)\n",
    "#         all_tokens = torch.cat((all_tokens, decoder_input), dim=0) # Record token and score\n",
    "#         all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "\n",
    "#         # Calculate and accumulate loss\n",
    "#         mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "#         loss += mask_loss\n",
    "#         print_losses.append(mask_loss.item() * nTotal)\n",
    "#         n_totals += nTotal\n",
    "    \n",
    "#     all_tokens = all_tokens.view(-1, max_target_len)\n",
    "#     all_scores = all_scores.view(-1, max_target_len)\n",
    "#     # print(all_tokens.shape, all_scores.shape)\n",
    "    \n",
    "#     # accuracy\n",
    "#     num_non_padding, num_correct = correct_tokens(all_tokens, target_variable, PAD_token)\n",
    "#     # acc = np.round(100 * (num_correct / num_non_padding), 2)\n",
    " \n",
    "#     return sum(print_losses) / n_totals, all_tokens, all_scores, num_non_padding, num_correct\n",
    "\n",
    "\n",
    "# from utils import MetricReporter\n",
    "# mc = MetricReporter(verbose=False)\n",
    "# mc.eval()\n",
    "# t0 = time.time()\n",
    "# with torch.no_grad():\n",
    "#     for batch_idx, batch in enumerate(valid_data_loader):\n",
    "#         v, q, a, mask, q_len, _ = batch\n",
    "\n",
    "#         out_loss, out_all_tokens, out_all_scores, out_num_non_padding, out_num_correct = eval(v, q, a, mask, q_len,\n",
    "#              MAX_A_LEN, MAX_Q_LEN + 1,\n",
    "#              encoder, decoder,\n",
    "#              batch_size)\n",
    "#         mc.update_metrics(out_loss, out_num_non_padding, out_num_correct)\n",
    "#         elapsed = format_time(time.time() - t0)\n",
    "\n",
    "#         # break\n",
    "#         mc.report_metrics()\n",
    "#         print('| VAL SET | Step [{:04d}/{:04d}],  Loss: {:.4f}, Accuracy: {:.4f} | Elapsed: {:}'\n",
    "#                               .format(batch_idx, int(n_val_batches), mc.list_valid_loss[-1], mc.list_valid_accuracy[-1], elapsed))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
